# Human Learning Machine Learning -- HLML
#### perception 
&nbsp; [python](https://github.com/zjn0505/ML/blob/master/Python/perceptron.py)

#### linear regression
- Cost Function
![LaTeX for Cost Function](http://www.sciweavers.org/upload/Tex2Img_1492691651/render.png)
- Gradient Descent for Multiple Variables

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ![LaTeX for Gradient Descent](http://www.sciweavers.org/upload/Tex2Img_1492581297/render.png)

<!--- LaTeX generated in http://www.sciweavers.org/free-online-latex-equation-editor -->
<!--- (\theta_j := \theta_j - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}-y^{(i)})x_j^{(i)})) -->

&nbsp; [python](https://github.com/zjn0505/ML/blob/master/Python/gradient_descent.py)

Q1: Is there a proper choice of learning rate and iteration so that we can say "Yes, we'd like to ensure J(Î˜) tend to converge at after 'X' iteration for most of input datasets"?

Note1: It seems to be a good habbit to declare necessary variables (vectors, matrices, return values) before using them. One will easily remember their dimensions in review.
